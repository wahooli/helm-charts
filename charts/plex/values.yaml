# Default values for openldap.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1
workloadType: Deployment

serviceMonitor:
  create: false
  endpoint:
    targetPort: 9000
    path: /metrics
    scheme: http
metrics:
  enabled: false
  image:
    repository: ghcr.io/jsclayton/prometheus-plex-exporter
  #   pullPolicy: IfNotPresent
  #   tag: "main"
# strategy:
#   type: Recreate

image:
  repository: ghcr.io/linuxserver/plex
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

hostname: plex
# runtimeClassName: nvidia
hostNetwork: false  # change to true to make visible on host network
# subdomain: localdomain
dnsPolicy: "ClusterFirstWithHostNet"
imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# env:
#   TZ: Etc/UTC

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  main:
    type: ClusterIP
    ports:
    - name: http
      port: 32400
      ingress: true
      protocol: TCP
    - name: dlna-udp
      port: 1900
      protocol: UDP
    - name: bonjour
      port: 5353
      protocol: UDP
    - name: plexcompanion
      port: 8324
      protocol: TCP
    - name: gdm1
      port: 32410
      protocol: UDP
    - name: gdm2
      port: 32412
      protocol: UDP
    - name: gdm3
      port: 32413
      protocol: UDP
    - name: gdm4
      port: 32414
      protocol: UDP
    - name: dlna-tcp
      port: 32469
      protocol: TCP
    - name: metrics-2
      serviceOnly: true
      port: 9594
      protocol: TCP

probe:
  liveness:
    enabled: true
    initialDelaySeconds: 6
    periodSeconds: 10
    timeoutSeconds: 1
    successThreshold: 1
    failureThreshold: 10
    httpGet:
      default: true
      path: /identity
      port: http

  readiness:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 1
    successThreshold: 1
    failureThreshold: 10
    httpGet:
      default: true
      path: /identity
      port: http

  startup:
    enabled: true
    initialDelaySeconds: 0
    periodSeconds: 10
    timeoutSeconds: 1
    successThreshold: 1
    failureThreshold: 30
    httpGet:
      default: true
      path: /identity
      port: http

persistence: {}
  # config:
  #   enabled: true
  #   mount:
  #   - path: /config
  #   spec:
  #     emptyDir: {}
  # movies:
  #   enabled: true
  #   mount:
  #   - path: /movies
  #   spec:
  #     hostPath:
  #       path: /path/to/movies
  #       type: Directory # DirectoryOrCreate
  # tv:
  #   enabled: true
  #   mount:
  #   - path: /tv
  #   spec:
  #     hostPath:
  #       path: /path/to/tv
  #       type: Directory # DirectoryOrCreate

ingress:
  main:
    enabled: false
    className: ""
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    hosts:
      - host: chart-example.local
        paths:
          - path: /
            pathType: ImplementationSpecific
        # servicePort: 80
        # serviceName: otherService
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}
